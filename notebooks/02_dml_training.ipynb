{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training Double Machine Learning (DML)\n",
    "\n",
    "This notebook implements a **Double Machine Learning (DML) Forecaster** that follows the method from  \n",
    "*Causal Forecasting for Pricing* (Schultz et al.). Instead of transformers, we use **LightGBM** models.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overall Idea\n",
    "\n",
    "We want to estimate how demand $q_t$ changes when we change the discount $d_t$, while controlling for  \n",
    "a set of covariates $z_t$ (time features, product features, lags, etc.).\n",
    "\n",
    "A naive regression of $q_t$ on $d_t$ is biased because discounts are **confounded** with these  \n",
    "covariates (seasonality, life-cycle, etc.). The DML Forecaster fixes this by:\n",
    "\n",
    "1. Learning what demand would be **without** using the current discount (`outcome model`).\n",
    "2. Learning how the **discount policy** behaves given covariates (`treatment model`).\n",
    "3. Learning an **elasticity function** $\\psi(z_t)$ on top of those two nuisance models.\n",
    "4. Using $\\psi(z_t)$ to simulate demand under new, counterfactual discounts.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Two Nuisance Models\n",
    "\n",
    "We first train two \"nuisance\" models:\n",
    "\n",
    "- **Outcome model**:  \n",
    "  $$\n",
    "  \\hat q_t(z) \\approx \\mathbb{E}[q_t \\mid z_t]\n",
    "  $$\n",
    "  Predicts demand from covariates, *without* using the current discount as an input.\n",
    "\n",
    "- **Treatment model**:  \n",
    "  $$\n",
    "  \\hat d_t(z) \\approx \\mathbb{E}[d_t \\mid z_t]\n",
    "  $$\n",
    "  Predicts the discount that the pricing policy would choose, given the same covariates.\n",
    "\n",
    "These models are trained with LightGBM regressors. They capture all the \"business as usual\" structure in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Sample-Splitting and Cross-Fitting\n",
    "\n",
    "To avoid overfitting and to get approximately unbiased residuals, we use **sample splitting**:\n",
    "\n",
    "1. Split items into two disjoint sets, e.g.\n",
    "   - **Even** item IDs\n",
    "   - **Odd** item IDs\n",
    "\n",
    "2. Train nuisance models on one half and predict on the other:\n",
    "   - Train on **even** items, predict $\\hat q_t, \\hat d_t$ for **odd** items.\n",
    "   - Train on **odd** items, predict $\\hat q_t, \\hat d_t$ for **even** items.\n",
    "\n",
    "This gives **cross-fitted predictions** $\\hat q_t$ and $\\hat d_t$ for every observation, where the model never sees the same item both in training and prediction.  \n",
    "These cross-fitted nuisance predictions are then treated as \"fixed\" when estimating the effect.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Effect Model: Learning Elasticity $\\psi(z)$\n",
    "\n",
    "The key quantity we want is a **context-dependent elasticity** $\\psi(z_t)$.\n",
    "\n",
    "Starting from the multiplicative pricing formula:\n",
    "\n",
    "$$\n",
    "\\tilde q_t(d_t) = \\hat q_t(z_t)\n",
    "\\left( \\frac{1 - d_t}{1 - \\hat d_t(z_t)} \\right)^{\\psi(z_t)},\n",
    "$$\n",
    "\n",
    "we take logs and rearrange to get a **pseudo-regression target**:\n",
    "\n",
    "$$\n",
    "\\log q_t - \\log \\hat q_t\n",
    "\\;\\approx\\;\n",
    "\\psi(z_t)\\,\n",
    "\\Big[\\,\\log(1 - d_t) - \\log(1 - \\hat d_t)\\,\\Big].\n",
    "$$\n",
    "\n",
    "Define:\n",
    "\n",
    "- **Numerator (response)**:\n",
    "  $$\n",
    "  y^{(\\psi)}_t\n",
    "  = \\log q_t - \\log \\hat q_t\n",
    "  $$\n",
    "- **Denominator (design scalar)**:\n",
    "  $$\n",
    "  x^{(\\psi)}_t\n",
    "  = \\log(1 - d_t) - \\log(1 - \\hat d_t)\n",
    "  $$\n",
    "\n",
    "Then, pointwise,\n",
    "\n",
    "$$\n",
    "\\psi(z_t) \\approx \\frac{y^{(\\psi)}_t}{x^{(\\psi)}_t}.\n",
    "$$\n",
    "\n",
    "In practice:\n",
    "\n",
    "1. We compute $y^{(\\psi)}_t$ and $x^{(\\psi)}_t$ using cross-fitted $\\hat q_t$ and $\\hat d_t$.\n",
    "2. We drop rows where $|x^{(\\psi)}_t|$ is too small (no meaningful price change ⇒ no elasticity signal).\n",
    "3. We form **labels**  \n",
    "   $$\n",
    "   \\tilde \\psi_t = \\frac{y^{(\\psi)}_t}{x^{(\\psi)}_t}\n",
    "   $$\n",
    "   and regress them on the covariates $z_t$ using another LightGBM regressor:\n",
    "   $$\n",
    "   \\hat\\psi(z_t) \\approx \\tilde \\psi_t.\n",
    "   $$\n",
    "\n",
    "This third model is the **effect head**: it estimates how sensitive demand is to relative price changes, conditional on covariates.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Final Prediction\n",
    "\n",
    "Once we have:\n",
    "\n",
    "- $\\hat q_t(z)$ – outcome model prediction\n",
    "- $\\hat d_t(z)$ – treatment model prediction\n",
    "- $\\hat\\psi(z)$ – elasticity estimate\n",
    "\n",
    "we can predict demand for any discount scenario $d^\\star_t$:\n",
    "\n",
    "$$\n",
    "\\tilde q_t(d^\\star_t)\n",
    "= \\hat q_t(z_t)\n",
    "\\left(\n",
    "  \\frac{1 - d^\\star_t}{1 - \\hat d_t(z_t)}\n",
    "\\right)^{\\hat\\psi(z_t)}.\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "- **On-policy prediction** (using the actually observed discount $d_t$):  \n",
    "  set $d^\\star_t = d_t$.\n",
    "\n",
    "- **Off-policy / counterfactual prediction** (e.g. \"What if we always discounted 30%?\"):  \n",
    "  set $d^\\star_t = 0.3$ for all rows, or pass any other scalar / vector of discounts.\n",
    "\n",
    "This gives a **causally-aware forecaster**: it uses all the structure learned in the nuisance models,  \n",
    "but adjusts demand as if we had chosen a different discount, using the learned elasticity $\\hat\\psi(z_t)$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the synthetic pricing data generated in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Databricks table\n",
    "from seaborn._core.typing import default\n",
    "\n",
    "\n",
    "catalog = \"causal_forecasting\"      # Change here\n",
    "schema = \"default\"                  # Change here\n",
    "\n",
    "table_name = f\"{catalog}.{schema}.synthetic_data\"\n",
    "df = spark.table(table_name).toPandas()\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Split data along time dimension (weeks 0-39 for training, weeks 40-59 for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"week\"] < 40].copy()\n",
    "test_df = df[df[\"week\"] >= 40].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df):,} rows\")\n",
    "print(f\"Test set: {len(test_df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Columns\n",
    "\n",
    "Covariate set Z_t (excludes current discount, but includes lag_discount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"week\",\n",
    "    \"base_price\",\n",
    "    \"category\",\n",
    "    \"k_category\",\n",
    "    \"season_type\",\n",
    "    \"lag_demand\",\n",
    "    \"lag_discount\",\n",
    "    \"week_sin\",\n",
    "    \"week_cos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMLForecasterLGBM:\n",
    "    \"\"\"\n",
    "    LGBM implementation of the DML Forecaster (Section 3, Eq. (6))\n",
    "    from 'Causal Forecasting for Pricing' (Schultz et al., 2024).\n",
    "\n",
    "    Structure:\n",
    "      - outcome nuisance model:   q_hat(z)\n",
    "      - treatment nuisance model: d_hat(z)\n",
    "      - effect model:             psi(z) = elasticity\n",
    "\n",
    "    Training:\n",
    "      1) Split items into even / odd sets (sample splitting).\n",
    "      2) Train nuisance models on even, predict on odd; then\n",
    "         train nuisance models on odd, predict on even\n",
    "         -> cross-fitted q_hat_cf, d_hat_cf for all rows.\n",
    "      3) Build elasticity labels from Eq. (6) in log form:\n",
    "           log(q) - log(q_hat_cf)\n",
    "           --------------------------------------------  ≈ psi(z)\n",
    "           log(1 - d) - log(1 - d_hat_cf)\n",
    "      4) Fit effect LGBM on z -> psi(z).\n",
    "\n",
    "    Prediction:\n",
    "      - Use both nuisance models (even/odd) on all rows and\n",
    "        average their predictions:\n",
    "          q_hat = (q_hat_even + q_hat_odd) / 2\n",
    "          d_hat = (d_hat_even + d_hat_odd) / 2\n",
    "      - Compute psi(z) from effect model.\n",
    "      - Apply Eq. (6) for desired discount scenario d*:\n",
    "          q_pred = q_hat * ((1 - d*) / (1 - d_hat)) ** psi(z)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        outcome_params=None,\n",
    "        treatment_params=None,\n",
    "        effect_params=None,\n",
    "        random_state: int = 0,\n",
    "        min_delta_log_price: float = 1e-3,\n",
    "    ):\n",
    "        # Default LGBM params (you can override via *_params)\n",
    "        base_params = {\n",
    "            \"n_estimators\": 200,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": -1,\n",
    "            \"random_state\": random_state,\n",
    "        }\n",
    "        def _merge(user):\n",
    "            p = base_params.copy()\n",
    "            if user is not None:\n",
    "                p.update(user)\n",
    "            return p\n",
    "\n",
    "        self.outcome_params = _merge(outcome_params)\n",
    "        self.treatment_params = _merge(treatment_params)\n",
    "        self.effect_params = _merge(effect_params)\n",
    "\n",
    "        self.random_state = random_state\n",
    "        self.min_delta_log_price = min_delta_log_price\n",
    "\n",
    "        # Will be populated in fit()\n",
    "        self.outcome_model_even_ = None\n",
    "        self.outcome_model_odd_ = None\n",
    "        self.treatment_model_even_ = None\n",
    "        self.treatment_model_odd_ = None\n",
    "        self.effect_model_ = None\n",
    "\n",
    "        self.feature_cols_ = None\n",
    "        self.treatment_col_ = None\n",
    "        self.outcome_col_ = None\n",
    "        self.item_col_ = None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Internal helpers\n",
    "    # ------------------------------------------------------------------\n",
    "    def _split_even_odd(self, df: pd.DataFrame):\n",
    "        # Split by item_id parity, like even/odd batches in the paper.\n",
    "        items = df[self.item_col_].unique()\n",
    "        items_even = set([i for i in items if int(i) % 2 == 0])\n",
    "        mask_even = df[self.item_col_].isin(items_even)\n",
    "        mask_odd = ~mask_even\n",
    "        return mask_even.values, mask_odd.values\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Fit\n",
    "    # ------------------------------------------------------------------\n",
    "    def fit(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        feature_cols,\n",
    "        treatment_col: str = \"discount\",\n",
    "        outcome_col: str = \"demand\",\n",
    "        item_col: str = \"item_id\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit nuisance and effect models following the DML Forecaster\n",
    "        approach (sample splitting + elasticity head).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Training data with columns outcome_col, treatment_col, item_col\n",
    "            and feature_cols (z_t).\n",
    "        feature_cols : list of str\n",
    "            Covariates z_t (no current discount; past discount OK).\n",
    "        \"\"\"\n",
    "        self.feature_cols_ = list(feature_cols)\n",
    "        self.treatment_col_ = treatment_col\n",
    "        self.outcome_col_ = outcome_col\n",
    "        self.item_col_ = item_col\n",
    "\n",
    "        # Ensure no NaNs for training\n",
    "        df = df.copy()\n",
    "        df = df.dropna(subset=self.feature_cols_ + [treatment_col, outcome_col, item_col])\n",
    "\n",
    "        X_all = df[self.feature_cols_].values\n",
    "        d_all = df[treatment_col].values.astype(float)\n",
    "        y_all = df[outcome_col].values.astype(float)\n",
    "\n",
    "        # 1) Split into even/odd item sets\n",
    "        mask_even, mask_odd = self._split_even_odd(df)\n",
    "        X_even, X_odd = X_all[mask_even], X_all[mask_odd]\n",
    "        y_even, y_odd = y_all[mask_even], y_all[mask_odd]\n",
    "        d_even, d_odd = d_all[mask_even], d_all[mask_odd]\n",
    "\n",
    "        # 2) Train nuisance models on even, predict on odd; and vice versa\n",
    "        # Outcome models\n",
    "        self.outcome_model_even_ = LGBMRegressor(**self.outcome_params)\n",
    "        self.outcome_model_even_.fit(X_even, y_even)\n",
    "\n",
    "        self.outcome_model_odd_ = LGBMRegressor(**self.outcome_params)\n",
    "        self.outcome_model_odd_.fit(X_odd, y_odd)\n",
    "\n",
    "        # Treatment models\n",
    "        self.treatment_model_even_ = LGBMRegressor(**self.treatment_params)\n",
    "        self.treatment_model_even_.fit(X_even, d_even)\n",
    "\n",
    "        self.treatment_model_odd_ = LGBMRegressor(**self.treatment_params)\n",
    "        self.treatment_model_odd_.fit(X_odd, d_odd)\n",
    "\n",
    "        # Cross-fitted nuisance predictions: q_hat_cf, d_hat_cf\n",
    "        q_hat_cf = np.zeros_like(y_all, dtype=float)\n",
    "        d_hat_cf = np.zeros_like(d_all, dtype=float)\n",
    "\n",
    "        # For odd rows, use models trained on even\n",
    "        q_hat_cf[mask_odd] = self.outcome_model_even_.predict(X_odd)\n",
    "        d_hat_cf[mask_odd] = self.treatment_model_even_.predict(X_odd)\n",
    "\n",
    "        # For even rows, use models trained on odd\n",
    "        q_hat_cf[mask_even] = self.outcome_model_odd_.predict(X_even)\n",
    "        d_hat_cf[mask_even] = self.treatment_model_odd_.predict(X_even)\n",
    "\n",
    "        # 3) Build effect training data: elasticity labels from Eq. (6) in log form\n",
    "        eps_q = 1e-3\n",
    "        eps_d = 1e-6\n",
    "\n",
    "        # log q and log q_hat\n",
    "        log_q = np.log(np.clip(y_all, eps_q, None))\n",
    "        log_q_hat = np.log(np.clip(q_hat_cf, eps_q, None))\n",
    "        num = log_q - log_q_hat\n",
    "\n",
    "        # log(1-d) and log(1-d_hat)\n",
    "        log_1_minus_d = np.log(np.clip(1.0 - d_all, eps_d, None))\n",
    "        log_1_minus_d_hat = np.log(np.clip(1.0 - d_hat_cf, eps_d, None))\n",
    "        den = log_1_minus_d - log_1_minus_d_hat\n",
    "\n",
    "        # Drop rows where denominator is ~0 (no price difference => no elasticity info)\n",
    "        mask_effect = np.abs(den) > self.min_delta_log_price\n",
    "        if mask_effect.sum() == 0:\n",
    "            raise RuntimeError(\"No rows with sufficient discount variation for effect model training.\")\n",
    "\n",
    "        psi_labels = num[mask_effect] / den[mask_effect]\n",
    "        X_effect = X_all[mask_effect]\n",
    "\n",
    "        # 4) Fit effect model psi(z)\n",
    "        self.effect_model_ = LGBMRegressor(**self.effect_params)\n",
    "        self.effect_model_.fit(X_effect, psi_labels)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Prediction\n",
    "    # ------------------------------------------------------------------\n",
    "    def predict(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        discount_scenario=None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict demand under a given discount scenario using Eq. (6).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Data with feature_cols, treatment_col, outcome_col, item_col.\n",
    "        discount_scenario : float, array-like, or None\n",
    "            - None: use logged discount from df[treatment_col_].\n",
    "            - scalar: use this discount for all rows.\n",
    "            - array-like: per-row discounts.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Predicted demand for each row.\n",
    "        \"\"\"\n",
    "        if any(m is None for m in [\n",
    "            self.outcome_model_even_,\n",
    "            self.outcome_model_odd_,\n",
    "            self.treatment_model_even_,\n",
    "            self.treatment_model_odd_,\n",
    "            self.effect_model_,\n",
    "        ]):\n",
    "            raise RuntimeError(\"Model not fitted. Call .fit() first.\")\n",
    "\n",
    "        df = df.copy()\n",
    "        df = df.dropna(subset=self.feature_cols_)\n",
    "\n",
    "        X = df[self.feature_cols_].values\n",
    "\n",
    "        # Nuisance predictions: average even/odd models (forecast-style)\n",
    "        q_hat_even = self.outcome_model_even_.predict(X)\n",
    "        q_hat_odd = self.outcome_model_odd_.predict(X)\n",
    "        q_hat = 0.5 * (q_hat_even + q_hat_odd)\n",
    "\n",
    "        d_hat_even = self.treatment_model_even_.predict(X)\n",
    "        d_hat_odd = self.treatment_model_odd_.predict(X)\n",
    "        d_hat = 0.5 * (d_hat_even + d_hat_odd)\n",
    "\n",
    "        # Effect model: psi(z)\n",
    "        psi_hat = self.effect_model_.predict(X)\n",
    "\n",
    "        # Desired discount scenario\n",
    "        if discount_scenario is None:\n",
    "            d_new = df[self.treatment_col_].values.astype(float)\n",
    "        else:\n",
    "            if np.isscalar(discount_scenario):\n",
    "                d_new = np.full(len(df), float(discount_scenario), dtype=float)\n",
    "            else:\n",
    "                d_new = np.asarray(discount_scenario, dtype=float)\n",
    "                if d_new.shape[0] != len(df):\n",
    "                    raise ValueError(\"discount_scenario has wrong length.\")\n",
    "\n",
    "        eps_d = 1e-6\n",
    "        # Eq. (6): q_hat * ((1 - d_new) / (1 - d_hat)) ** psi_hat\n",
    "        ratio = np.clip(1.0 - d_new, eps_d, None) / np.clip(1.0 - d_hat, eps_d, None)\n",
    "        q_pred = np.clip(q_hat, 1e-3, None) * (ratio ** psi_hat)\n",
    "\n",
    "        return q_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DML Forecaster\n",
    "\n",
    "Instantiate and train the DML forecaster with 2-fold cross-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = DMLForecasterLGBM(\n",
    "    outcome_params={\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": -1},\n",
    "    treatment_params={\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": -1},\n",
    "    effect_params={\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": -1},\n",
    "    n_folds=2,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "forecaster.fit(\n",
    "    train_df,\n",
    "    feature_cols=feature_cols,\n",
    "    treatment_col=\"discount\",\n",
    "    outcome_col=\"demand\",\n",
    "    item_col=\"item_id\",\n",
    ")\n",
    "\n",
    "print(\"DML forecaster trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate: On-Policy Predictions\n",
    "\n",
    "Evaluate the model using the actual discounts in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df[\"demand\"].values\n",
    "y_pred_on = forecaster.predict(test_df)\n",
    "\n",
    "mae_on = mean_absolute_error(y_true, y_pred_on)\n",
    "mse_on = mean_squared_error(y_true, y_pred_on)\n",
    "rmse_on = np.sqrt(mse_on)\n",
    "\n",
    "print(\"On-policy performance (using logged discounts):\")\n",
    "print(f\"  MAE = {mae_on:.3f}\")\n",
    "print(f\"  RMSE = {rmse_on:.3f}\")\n",
    "print(f\"  MSE = {mse_on:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate: Off-Policy Predictions\n",
    "\n",
    "Evaluate counterfactual scenarios with different discount levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disc in [0.1, 0.3, 0.5]:\n",
    "    y_pred_off = forecaster.predict(test_df, discount_scenario=disc)\n",
    "    mae_off = mean_absolute_error(y_true, y_pred_off)\n",
    "    mse_off = mean_squared_error(y_true, y_pred_off)\n",
    "    rmse_off = np.sqrt(mse_off)\n",
    "    print(f\"\\nOff-policy performance (counterfactual discount = {disc:.1f}):\")\n",
    "    print(f\"  MAE vs actual realized demand = {mae_off:.3f}\")\n",
    "    print(f\"  RMSE vs actual realized demand = {rmse_off:.3f}\")\n",
    "    print(f\"  MSE vs actual realized demand = {mse_off:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand Sensitivity Analysis\n",
    "\n",
    "Inspect how average predicted demand changes with different discount levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average predicted demand at different discount levels:\\n\")\n",
    "for disc in [0.0, 0.2, 0.4, 0.6]:\n",
    "    y_pred = forecaster.predict(test_df, discount_scenario=disc)\n",
    "    print(f\"  Discount {disc:.1f}: {y_pred.mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
