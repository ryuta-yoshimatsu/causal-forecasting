{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Pricing Data\n",
    "\n",
    "This notebook generates synthetic panel data for causal forecasting with pricing.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Time-varying base demand (trend + seasonality)\n",
    "- Constant item-level price elasticity\n",
    "- Confounded pricing policy: discounts depend on latent demand\n",
    "- Saves data to Databricks table for downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the catalog and the schema exist\n",
    "\n",
    "catalog = \"causal_forecasting\"      # Change here\n",
    "schema = \"default\"                  # Change here\n",
    "\n",
    "_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\") \n",
    "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Generator\n",
    "\n",
    "The function below generates panel data with:\n",
    "- Item-level characteristics (base_price, category, season_type)\n",
    "- Time-varying demand with trend and seasonality\n",
    "- Confounded treatment: discounts are higher when base demand is low\n",
    "- Constant price elasticity per item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_synthetic_pricing_data(\n",
    "    n_items: int = 200,\n",
    "    n_weeks: int = 60,\n",
    "    seed: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Synthetic panel data {i, t} inspired by the simulation in\n",
    "    'Causal Forecasting for Pricing' (Schultz et al., 2024).\n",
    "\n",
    "    Demand model:\n",
    "        base_demand_{i,t} from Eq. (14):\n",
    "            q_b(i,t) = (0.15 * tau_{i,t} + 0.25 * s_{i,t} + 1) * c_{i,t}\n",
    "        observed demand (slightly modified):\n",
    "            q_{i,t} ~ Poisson( q_b(i,t) * (p_{i,t} / p0_i)^{elasticity_i} )\n",
    "\n",
    "    Where:\n",
    "      - tau_{i,t} ~ N(t * gamma_i, sigma_tau_i^2)\n",
    "      - s_{i,t} is a sine with period 30 and article-specific phase (season type)\n",
    "      - c_{i,t} = 0.05 * a_{i,t}^2 + 0.25 * a_{i,t} + 0.5 * b_{i,t}\n",
    "      - a_{i,t} = alpha_{d(i)} + eps_{i,t}, alpha_d ~ N(10, 3^2), eps_{i,t} ~ N(0,1)\n",
    "      - b_{i,t} = beta_{k(i)} + psi_{i,t}, beta_k ~ N(300, 50^2), psi_{i,t} ~ N(0,5^2)\n",
    "\n",
    "    Pricing policy:\n",
    "      - discrete discounts in {0, 0.1, ..., 0.5}\n",
    "      - higher discounts when current base_demand is low (confounding).\n",
    "\n",
    "    Columns returned:\n",
    "      item_id, week, demand, discount, base_price,\n",
    "      category (d-category 0..44), \n",
    "      k_category (k-category, 0..14),\n",
    "      season_type (0..5),\n",
    "      price, lag_demand, lag_discount,\n",
    "      week_sin, week_cos,\n",
    "      elasticity_true, base_demand\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    # ----- Synthetic \"meta\" parameters as in Appendix E -----\n",
    "    # a_it categories: d(i) in {0,...,44}, alpha_d ~ N(10, 3^2)\n",
    "    n_cat_a = 45\n",
    "    alpha_d = rng.normal(loc=10.0, scale=3.0, size=n_cat_a)\n",
    "\n",
    "    # b_it categories: k(i) in {0,...,14}, beta_k ~ N(300, 50^2)\n",
    "    n_cat_b = 15\n",
    "    beta_k = rng.normal(loc=300.0, scale=50.0, size=n_cat_b)\n",
    "\n",
    "    # Seasonality groups (6 \"season types\" tied to k(i))\n",
    "    n_season_types = 6\n",
    "    # Assign each k to one of 6 groups (as in \"subdivide k evenly into six subgroups\")\n",
    "    season_type_for_k = np.repeat(np.arange(n_season_types),\n",
    "                                  repeats=int(np.ceil(n_cat_b / n_season_types)))[:n_cat_b]\n",
    "    # Season shift (integer weeks) per season type, uniform over [-15, 15]\n",
    "    season_shift_by_type = rng.integers(-15, 16, size=n_season_types)\n",
    "\n",
    "    # Allowed discrete discount values d^(j) = 0.1 * j, j in {0,...,5}\n",
    "    allowed_discounts = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "    for i in range(n_items):\n",
    "        item_id = i\n",
    "\n",
    "        # Map article to synthetic categories d(i) and k(i)\n",
    "        d_cat = rng.integers(0, n_cat_a)  # for a_it\n",
    "        k_cat = rng.integers(0, n_cat_b)  # for b_it\n",
    "\n",
    "        # Season type determined by k(i) subgroup\n",
    "        season_type = season_type_for_k[k_cat]\n",
    "        season_shift = season_shift_by_type[season_type]\n",
    "\n",
    "        # Base (undiscounted) price p0 (we keep it simple, not exactly Eq. (21))\n",
    "        base_price = rng.uniform(20.0, 100.0)\n",
    "\n",
    "        # Trend parameters: gamma_i ~ U([-0.02, 0.02]), sigma_tau_i ~ U([0, 0.15])\n",
    "        gamma_i = rng.uniform(-0.02, 0.02)\n",
    "        sigma_tau_i = rng.uniform(0.0, 0.15)\n",
    "\n",
    "        # Constant per-item elasticity (not in the original synthetic data generation, \n",
    "        # but aligned with the elasticity framework in Appendix B)\n",
    "        elasticity = rng.uniform(-3.0, -0.5)\n",
    "\n",
    "        # For lag features\n",
    "        last_q = 0.0\n",
    "        last_d = 0.0\n",
    "\n",
    "        # Baseline base demand (for normalizing in the pricing policy)\n",
    "        base_demand_baseline = None\n",
    "\n",
    "        for t in range(n_weeks):\n",
    "            # ---------- Article-specific component c_{i,t} (Eq. (15)-(19)) ----------\n",
    "            # a_it = alpha_{d(i)} + eps_it, eps_it ~ N(0,1)\n",
    "            eps_it = rng.normal(0.0, 1.0)\n",
    "            a_it = alpha_d[d_cat] + eps_it\n",
    "\n",
    "            # b_it = beta_{k(i)} + psi_it, psi_it ~ N(0, 5^2)\n",
    "            psi_it = rng.normal(0.0, 5.0)\n",
    "            b_it = beta_k[k_cat] + psi_it\n",
    "\n",
    "            # c_it = 0.05 * a_it^2 + 0.25 * a_it + 0.5 * b_it (Eq. (15))\n",
    "            c_it = 0.05 * (a_it ** 2) + 0.25 * a_it + 0.5 * b_it\n",
    "\n",
    "            # ---------- Trend tau_{i,t} (Eq. (22)-(24)) ----------\n",
    "            # tau_it ~ N(t * gamma_i, sigma_tau_i^2)\n",
    "            tau_it = rng.normal(loc=t * gamma_i, scale=sigma_tau_i)\n",
    "\n",
    "            # ---------- Seasonality s_{i,t} ----------\n",
    "            # Sine with period 30 and article-dependent shift (as in Appendix E)\n",
    "            s_it = np.sin(2.0 * np.pi * (t + season_shift) / 30.0)\n",
    "\n",
    "            # ---------- Base demand q^{(b)}_{i,t} (Eq. (14)) ----------\n",
    "            # q_b_it = (0.15 * tau_it + 0.25 * s_it + 1) * c_it\n",
    "            base_demand = (0.15 * tau_it + 0.25 * s_it + 1.0) * c_it\n",
    "\n",
    "            # Store baseline for pricing policy normalization (t = 0)\n",
    "            if base_demand_baseline is None:\n",
    "                base_demand_baseline = base_demand\n",
    "\n",
    "            # ---------- Pricing policy (confounding, only discount when demand is far below baseline) ----------\n",
    "            # norm_bd measures how far current base_demand is from the initial base_demand_baseline\n",
    "            norm_bd = (base_demand - base_demand_baseline) / (\n",
    "                0.5 * base_demand_baseline + 1e-6\n",
    "            )\n",
    "\n",
    "            # Define \"far below baseline\" as below -1.0 in normalized units\n",
    "            #   norm_bd ≈ 0   → demand similar to baseline\n",
    "            #   norm_bd > 0   → demand above baseline\n",
    "            #   norm_bd < 0   → demand below baseline\n",
    "            threshold = -0.5  # roughly: current base_demand < ~50% of baseline\n",
    "\n",
    "            if norm_bd >= threshold:\n",
    "                # Demand is not far below baseline → no discount\n",
    "                disc_mean_raw = 0.0\n",
    "            else:\n",
    "                # Demand is far below baseline → introduce a positive discount\n",
    "                # shortfall is how much further below the threshold we are\n",
    "                shortfall = -(norm_bd - threshold)  # = -norm_bd - 0.5  when threshold = -0.5\n",
    "\n",
    "                # Scale shortfall\n",
    "                disc_mean_raw = np.clip(2.0 * shortfall, 0.0, 0.5)\n",
    "\n",
    "            # Add small noise so we don't get perfectly deterministic discounts\n",
    "            disc_mean = float(np.clip(disc_mean_raw, 0.0, 0.5))\n",
    "            discount_continuous = float(\n",
    "                np.clip(disc_mean + rng.normal(0.0, 0.03), 0.0, 0.5)\n",
    "            )\n",
    "\n",
    "            # Discretize to {0, 0.1, 0.2, 0.3, 0.4, 0.5}, as in the paper\n",
    "            discount = float(\n",
    "                allowed_discounts[\n",
    "                    np.argmin(np.abs(allowed_discounts - discount_continuous))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Price under this discount: p_t = p0 * (1 - d_t)\n",
    "            price = base_price * (1.0 - discount)\n",
    "\n",
    "            # ---------- Constant elasticity demand on top of q^{(b)} ----------\n",
    "            # expected_q = q_b_it * (p_t / p0)^{elasticity_i}\n",
    "            expected_q = base_demand * ((price / base_price) ** elasticity)\n",
    "            expected_q = max(expected_q, 1e-3)\n",
    "\n",
    "            # Poisson noise for integer demand\n",
    "            demand = float(rng.poisson(expected_q))\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"item_id\": item_id,\n",
    "                    \"week\": t,\n",
    "                    \"demand\": demand,\n",
    "                    \"discount\": discount,\n",
    "                    \"base_price\": base_price,\n",
    "                    # For interpretability / alignment with paper:\n",
    "                    \"category\": int(d_cat),        # corresponds to d(i) in Eq. (16)-(17)\n",
    "                    \"k_category\": int(k_cat),        # k(i)\n",
    "                    \"season_type\": int(season_type),  # group that defines season shift\n",
    "                    \"elasticity_true\": elasticity,  # our per-item epsilon (not in original DGP)\n",
    "                    \"base_demand\": base_demand,    # q^{(b)}_{i,t}\n",
    "                    \"price\": price,\n",
    "                    \"lag_demand\": last_q,\n",
    "                    \"lag_discount\": last_d,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            last_q = demand\n",
    "            last_d = discount\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Add simple periodic time features (proxy for positional encodings)\n",
    "    df[\"week_sin\"] = np.sin(2 * np.pi * df[\"week\"] / 30.0)\n",
    "    df[\"week_cos\"] = np.cos(2 * np.pi * df[\"week\"] / 30.0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "Generate 200 items over 60 weeks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_synthetic_pricing_data(\n",
    "    n_items=2000,\n",
    "    n_weeks=60,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(df):,} rows\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Time Series\n",
    "\n",
    "Let's visualize the time series for 5 randomly sampled items to understand the data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set modern seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "\n",
    "# Sample 10 random items\n",
    "np.random.seed(42)\n",
    "sample_items = np.random.choice(df['item_id'].unique(), size=5, replace=False)\n",
    "\n",
    "# Create figure with enhanced styling\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 11))\n",
    "\n",
    "# Use colorblind-friendly palette\n",
    "colors = sns.color_palette(\"husl\", 10)\n",
    "\n",
    "# ===== Plot 1: Demand Over Time =====\n",
    "ax1 = axes[0]\n",
    "for idx, item_id in enumerate(sample_items):\n",
    "    item_data = df[df['item_id'] == item_id].sort_values('week')\n",
    "    ax1.plot(item_data['week'], item_data['demand'], \n",
    "             label=f'Item {item_id}', \n",
    "             color=colors[idx], \n",
    "             linewidth=2.5, \n",
    "             alpha=0.85,\n",
    "             marker='o',\n",
    "             markersize=4,\n",
    "             markevery=5)\n",
    "\n",
    "ax1.set_xlabel('Week', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Demand (units)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Demand Over Time - Sample of 5 Items', \n",
    "              fontsize=15, fontweight='bold', pad=15)\n",
    "ax1.legend(loc='best', ncol=2, framealpha=0.95, \n",
    "           edgecolor='gray', fancybox=True, shadow=True)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# ===== Plot 2: Discount Over Time (Step Plot) =====\n",
    "ax2 = axes[1]\n",
    "for idx, item_id in enumerate(sample_items):\n",
    "    item_data = df[df['item_id'] == item_id].sort_values('week')\n",
    "    ax2.step(item_data['week'], item_data['discount'], \n",
    "             label=f'Item {item_id}', \n",
    "             color=colors[idx], \n",
    "             linewidth=2.5, \n",
    "             alpha=0.85,\n",
    "             where='post')\n",
    "\n",
    "ax2.set_xlabel('Week', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Discount Rate', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Discount Rate Over Time - Discrete Levels [0, 0.1, 0.2, 0.3, 0.4, 0.5]', \n",
    "              fontsize=15, fontweight='bold', pad=15)\n",
    "ax2.legend(loc='best', ncol=2, framealpha=0.95, \n",
    "           edgecolor='gray', fancybox=True, shadow=True)\n",
    "ax2.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "# Add horizontal reference lines for discount levels\n",
    "for discount_level in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    ax2.axhline(y=discount_level, color='gray', linestyle=':', \n",
    "                alpha=0.3, linewidth=1, zorder=0)\n",
    "\n",
    "# Overall figure title\n",
    "fig.suptitle('Synthetic Pricing Data: Time Series Visualization', \n",
    "             fontsize=17, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Databricks Table\n",
    "\n",
    "Save the generated data to a Databricks table for use in downstream training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame and save\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Save to table (adjust catalog/schema as needed)\n",
    "table_name = f\"{catalog}.{schema}.synthetic_data\"\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"Data saved to table: {table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
