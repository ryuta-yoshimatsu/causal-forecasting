{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "template-md-cell",
   "metadata": {},
   "source": [
    "# Causal Forecasting with Databricks\n",
    "\n",
    "This notebook demonstrates Double Machine Learning (DML) forecasting with LightGBM on Databricks.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Databricks connection and authentication\n",
    "- Double Machine Learning for causal forecasting\n",
    "- LightGBM-based nuisance and effect models\n",
    "- Synthetic data generation for testing\n",
    "- On-policy and off-policy predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "template-code-cell",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/ryuta.yoshimatsu/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/interval.cpython-312-darwin.so, 0x0002): tried: '/Users/ryuta.yoshimatsu/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/interval.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/ryuta.yoshimatsu/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/interval.cpython-312-darwin.so' (no such file), '/Users/ryuta.yoshimatsu/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/interval.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mDouble Machine Learning Forecaster with LightGBM\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m================================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33;03mWe treat this as a one-step-ahead forecasting problem.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/__init__.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/compat/__init__.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     is_numpy_dev,\n\u001b[32m     20\u001b[39m     np_version_under1p21,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     pa_version_under1p01,\n\u001b[32m     24\u001b[39m     pa_version_under2p0,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     pa_version_under9p0,\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/compat/numpy/__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[32m      7\u001b[39m _np_version = np.__version__\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/util/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      3\u001b[39m     Appender,\n\u001b[32m      4\u001b[39m     Substitution,\n\u001b[32m      5\u001b[39m     cache_readonly,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m     hash_array,\n\u001b[32m     10\u001b[39m     hash_pandas_object,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/util/_decorators.py:14\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     Any,\n\u001b[32m      8\u001b[39m     Callable,\n\u001b[32m      9\u001b[39m     Mapping,\n\u001b[32m     10\u001b[39m     cast,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     F,\n\u001b[32m     17\u001b[39m     T,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m __all__ = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNaT\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNaTType\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mInterval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     NaT,\n\u001b[32m     16\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     iNaT,\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: dlopen(/Users/ryuta.yoshimatsu/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/interval.cpython-312-darwin.so, 0x0002): tried: '/Users/ryuta.yoshimatsu/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/interval.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/ryuta.yoshimatsu/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/interval.cpython-312-darwin.so' (no such file), '/Users/ryuta.yoshimatsu/miniconda3/envs/databricks-ml/lib/python3.12/site-packages/pandas/_libs/interval.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Double Machine Learning Forecaster with LightGBM\n",
    "================================================\n",
    "\n",
    "Implements a simplified version of the DML forecaster from\n",
    "\"Causal Forecasting for Pricing\" (Schultz et al., 2024),\n",
    "but using LightGBM instead of transformer models.\n",
    "\n",
    "Key ideas (matching the paper):\n",
    "- Nuisance models:\n",
    "    * Outcome model:   q_t  ≈ E[q_t | Z_t]\n",
    "    * Treatment model: d_t  ≈ E[d_t | Z_t]\n",
    "  where Z_t are covariates / confounders (no current d_t).\n",
    "- Effect model:\n",
    "    * Learns E[q_t - q_hat_t | d_t - d_hat_t, Z_t]\n",
    "- Final prediction for a desired discount d*:\n",
    "    q*(d*) = q_hat(Z) + effect_model(d* - d_hat(Z), Z)\n",
    "\n",
    "We treat this as a one-step-ahead forecasting problem.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Synthetic dataset generator (pricing + demand + confounding)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def generate_synthetic_pricing_data(\n",
    "    n_items: int = 200,\n",
    "    n_weeks: int = 60,\n",
    "    seed: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic panel data {i, t} with:\n",
    "      - Time-varying base demand (trend + seasonality)\n",
    "      - Constant item-level price elasticity\n",
    "      - Confounded pricing policy: discounts depend on latent demand\n",
    "\n",
    "    Columns returned:\n",
    "      item_id, week, demand, discount, base_price, category, season_type,\n",
    "      price, lag_demand, lag_discount, week_sin, week_cos, elasticity_true\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    for i in range(n_items):\n",
    "        item_id = i\n",
    "        base_price = rng.uniform(20, 100)        # \"black price\"\n",
    "        category = rng.integers(0, 5)            # 5 categories\n",
    "        season_type = rng.integers(0, 4)         # 4 season types\n",
    "        gamma = rng.uniform(-0.05, 0.05)         # trend slope\n",
    "        amp = rng.uniform(0.5, 1.5)              # seasonal amplitude\n",
    "        phase = rng.uniform(0, 2 * np.pi)        # seasonal phase\n",
    "        elasticity = rng.uniform(-3.0, -0.5)     # constant, negative\n",
    "\n",
    "        base_level = rng.uniform(20, 80)         # base scale\n",
    "\n",
    "        last_q = base_level                      # for lag feature\n",
    "        last_d = 0.0                             # for lag feature\n",
    "\n",
    "        for t in range(n_weeks):\n",
    "            # deterministic trend + seasonality for base demand\n",
    "            season = amp * np.sin(2 * np.pi * (t / 30.0) + phase)\n",
    "            trend = gamma * t\n",
    "            base_demand = np.exp(np.log(base_level) + trend) * (1 + 0.3 * season)\n",
    "\n",
    "            # Pricing policy (confounding):\n",
    "            # - when base demand is low, discount more.\n",
    "            norm_bd = (base_demand - base_level) / (0.5 * base_level + 1e-6)\n",
    "            disc_mean = np.clip(0.3 - 0.15 * norm_bd, 0.0, 0.6)\n",
    "            discount = np.clip(disc_mean + rng.normal(0, 0.05), 0.0, 0.6)\n",
    "\n",
    "            price = base_price * (1.0 - discount)\n",
    "\n",
    "            # Constant elasticity demand:\n",
    "            # q_t = base_demand_t * (p_t / base_price)^{elasticity}\n",
    "            expected_q = base_demand * ((price / base_price) ** elasticity)\n",
    "            expected_q = max(expected_q, 1e-3)\n",
    "\n",
    "            # Poisson noise around expected demand\n",
    "            demand = rng.poisson(expected_q)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"item_id\": item_id,\n",
    "                    \"week\": t,\n",
    "                    \"demand\": float(demand),\n",
    "                    \"discount\": float(discount),\n",
    "                    \"base_price\": base_price,\n",
    "                    \"category\": category,\n",
    "                    \"season_type\": season_type,\n",
    "                    \"elasticity_true\": elasticity,\n",
    "                    \"base_demand\": base_demand,\n",
    "                    \"price\": price,\n",
    "                    \"lag_demand\": last_q,\n",
    "                    \"lag_discount\": last_d,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            last_q = demand\n",
    "            last_d = discount\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Add periodic time features (like in the paper’s time embeddings)\n",
    "    df[\"week_sin\"] = np.sin(2 * np.pi * df[\"week\"] / 30.0)\n",
    "    df[\"week_cos\"] = np.cos(2 * np.pi * df[\"week\"] / 30.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. DML Forecaster using LightGBM\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "class DMLForecasterLGBM:\n",
    "    \"\"\"\n",
    "    Double Machine Learning forecaster with LightGBM:\n",
    "\n",
    "    Stage 1 (nuisance models, trained via cross-fitting):\n",
    "        outcome_model:   y  ~ Z    (predict demand without using current discount)\n",
    "        treatment_model: d  ~ Z    (predict discount from Z)\n",
    "\n",
    "    Stage 2 (effect model):\n",
    "        Residuals:   y_resid = y - y_hat\n",
    "                     d_resid = d - d_hat\n",
    "        effect_model: y_resid ~ [d_resid, Z]\n",
    "\n",
    "    Prediction for arbitrary discount scenario d*:\n",
    "        1. Compute y_hat(Z), d_hat(Z) using nuisance models\n",
    "        2. d_resid* = d* - d_hat(Z)\n",
    "        3. delta_y  = effect_model.predict([d_resid*, Z])\n",
    "        4. y_pred   = y_hat(Z) + delta_y\n",
    "\n",
    "    This mimics Eq. (3) and the orthogonalization idea in the paper,\n",
    "    but uses flexible LightGBM models instead of transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        outcome_params=None,\n",
    "        treatment_params=None,\n",
    "        effect_params=None,\n",
    "        n_folds: int = 2,\n",
    "        random_state: int = 0,\n",
    "    ):\n",
    "        self.outcome_params = outcome_params or {\n",
    "            \"n_estimators\": 200,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": -1,\n",
    "        }\n",
    "        self.treatment_params = treatment_params or {\n",
    "            \"n_estimators\": 200,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": -1,\n",
    "        }\n",
    "        self.effect_params = effect_params or {\n",
    "            \"n_estimators\": 200,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": -1,\n",
    "        }\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.outcome_models_ = []\n",
    "        self.treatment_models_ = []\n",
    "        self.effect_model_ = None\n",
    "\n",
    "        self.feature_cols_ = None\n",
    "        self.treatment_col_ = None\n",
    "        self.outcome_col_ = None\n",
    "        self.item_col_ = None\n",
    "\n",
    "    # ------------ helpers ------------\n",
    "\n",
    "    def _build_folds(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Build folds for cross-fitting. If an item_id column is given,\n",
    "        we split by items (as in the paper: even/odd articles).\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "\n",
    "        if self.item_col_ is not None:\n",
    "            items = df[self.item_col_].unique()\n",
    "            rng.shuffle(items)\n",
    "            folds_items = np.array_split(items, self.n_folds)\n",
    "            index_folds = []\n",
    "            for items_fold in folds_items:\n",
    "                mask = df[self.item_col_].isin(items_fold).values\n",
    "                index_folds.append(np.where(mask)[0])\n",
    "            return index_folds\n",
    "        else:\n",
    "            n = len(df)\n",
    "            indices = np.arange(n)\n",
    "            rng.shuffle(indices)\n",
    "            return np.array_split(indices, self.n_folds)\n",
    "\n",
    "    # ------------ API ------------\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        feature_cols,\n",
    "        treatment_col: str,\n",
    "        outcome_col: str,\n",
    "        item_col: str | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit the DML forecaster.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Training data.\n",
    "        feature_cols : list of str\n",
    "            Names of covariate columns Z (no current treatment).\n",
    "        treatment_col : str\n",
    "            Name of treatment column d (discount).\n",
    "        outcome_col : str\n",
    "            Name of outcome column q (demand).\n",
    "        item_col : str, optional\n",
    "            Column identifying items; used for panel-wise cross-fitting.\n",
    "        \"\"\"\n",
    "        self.feature_cols_ = list(feature_cols)\n",
    "        self.treatment_col_ = treatment_col\n",
    "        self.outcome_col_ = outcome_col\n",
    "        self.item_col_ = item_col\n",
    "\n",
    "        X = df[self.feature_cols_].values\n",
    "        d = df[treatment_col].values.astype(float)\n",
    "        y = df[outcome_col].values.astype(float)\n",
    "        n = len(df)\n",
    "\n",
    "        # Out-of-fold nuisance predictions\n",
    "        y_hat = np.zeros(n)\n",
    "        d_hat = np.zeros(n)\n",
    "\n",
    "        folds = self._build_folds(df)\n",
    "\n",
    "        for val_idx in folds:\n",
    "            train_idx = np.setdiff1d(np.arange(n), val_idx)\n",
    "\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            d_train, d_val = d[train_idx], d[val_idx]\n",
    "\n",
    "            outcome_model = LGBMRegressor(\n",
    "                random_state=self.random_state, **self.outcome_params\n",
    "            )\n",
    "            treatment_model = LGBMRegressor(\n",
    "                random_state=self.random_state, **self.treatment_params\n",
    "            )\n",
    "\n",
    "            # Outcome nuisance: E[q | Z]\n",
    "            outcome_model.fit(X_train, y_train)\n",
    "            # Treatment nuisance: E[d | Z]\n",
    "            treatment_model.fit(X_train, d_train)\n",
    "\n",
    "            y_hat[val_idx] = outcome_model.predict(X_val)\n",
    "            d_hat[val_idx] = treatment_model.predict(X_val)\n",
    "\n",
    "            self.outcome_models_.append(outcome_model)\n",
    "            self.treatment_models_.append(treatment_model)\n",
    "\n",
    "        # Residuals for the effect model\n",
    "        y_resid = y - y_hat\n",
    "        d_resid = d - d_hat\n",
    "        effect_features = np.column_stack([d_resid.reshape(-1, 1), X])\n",
    "\n",
    "        self.effect_model_ = LGBMRegressor(\n",
    "            random_state=self.random_state, **self.effect_params\n",
    "        )\n",
    "        self.effect_model_.fit(effect_features, y_resid)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict_nuisances(self, df: pd.DataFrame):\n",
    "        \"\"\"Average over nuisance models for inference (as in an ensemble).\"\"\"\n",
    "        X = df[self.feature_cols_].values\n",
    "        y_hats = []\n",
    "        d_hats = []\n",
    "        for outcome_model, treatment_model in zip(\n",
    "            self.outcome_models_, self.treatment_models_\n",
    "        ):\n",
    "            y_hats.append(outcome_model.predict(X))\n",
    "            d_hats.append(treatment_model.predict(X))\n",
    "        y_hat = np.mean(np.stack(y_hats, axis=0), axis=0)\n",
    "        d_hat = np.mean(np.stack(d_hats, axis=0), axis=0)\n",
    "        return y_hat, d_hat\n",
    "\n",
    "    def predict(self, df: pd.DataFrame, discount_scenario=None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict demand under a given discount scenario.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Data containing covariates Z (and possibly current discount).\n",
    "        discount_scenario :\n",
    "            - None: use df[treatment_col_] (on-policy predictions)\n",
    "            - float: use this fixed discount for all rows\n",
    "            - array-like (len(df)): per-row discounts\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Predicted demand.\n",
    "        \"\"\"\n",
    "        if self.effect_model_ is None:\n",
    "            raise RuntimeError(\"Call fit() before predict().\")\n",
    "\n",
    "        # Nuisance predictions\n",
    "        y_hat, d_hat = self._predict_nuisances(df)\n",
    "\n",
    "        # Decide which discount to use\n",
    "        if discount_scenario is None:\n",
    "            d_new = df[self.treatment_col_].values.astype(float)\n",
    "        else:\n",
    "            if np.isscalar(discount_scenario):\n",
    "                d_new = np.full(len(df), float(discount_scenario), dtype=float)\n",
    "            else:\n",
    "                d_new = np.asarray(discount_scenario, dtype=float)\n",
    "                if d_new.shape[0] != len(df):\n",
    "                    raise ValueError(\"discount_scenario has wrong length.\")\n",
    "\n",
    "        d_resid_new = d_new - d_hat\n",
    "        X = df[self.feature_cols_].values\n",
    "        effect_features_new = np.column_stack([d_resid_new.reshape(-1, 1), X])\n",
    "\n",
    "        delta_y = self.effect_model_.predict(effect_features_new)\n",
    "        y_pred = y_hat + delta_y\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Example usage\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 3.1 Generate synthetic data\n",
    "    df = generate_synthetic_pricing_data(\n",
    "        n_items=200,\n",
    "        n_weeks=60,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Train / test split along time (like in the paper)\n",
    "    train_df = df[df[\"week\"] < 40].copy()\n",
    "    test_df = df[df[\"week\"] >= 40].copy()\n",
    "\n",
    "    # Covariate set Z_t (no current discount; we *do* include lag_discount)\n",
    "    feature_cols = [\n",
    "        \"week\",\n",
    "        \"base_price\",\n",
    "        \"category\",\n",
    "        \"season_type\",\n",
    "        \"lag_demand\",\n",
    "        \"lag_discount\",\n",
    "        \"week_sin\",\n",
    "        \"week_cos\",\n",
    "    ]\n",
    "\n",
    "    # 3.2 Instantiate and train the DML forecaster\n",
    "    forecaster = DMLForecasterLGBM(\n",
    "        outcome_params={\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": -1},\n",
    "        treatment_params={\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": -1},\n",
    "        effect_params={\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": -1},\n",
    "        n_folds=2,\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "    forecaster.fit(\n",
    "        train_df,\n",
    "        feature_cols=feature_cols,\n",
    "        treatment_col=\"discount\",\n",
    "        outcome_col=\"demand\",\n",
    "        item_col=\"item_id\",\n",
    "    )\n",
    "\n",
    "    # 3.3 On-policy prediction: use actual discounts in the test set\n",
    "    y_true = test_df[\"demand\"].values\n",
    "    y_pred_on = forecaster.predict(test_df)\n",
    "\n",
    "    mae_on = mean_absolute_error(y_true, y_pred_on)\n",
    "    mse_on = mean_squared_error(y_true, y_pred_on)\n",
    "\n",
    "    print(\"On-policy performance (using logged discounts):\")\n",
    "    print(f\"  MAE = {mae_on:.3f}\")\n",
    "    print(f\"  MSE = {mse_on:.3f}\")\n",
    "\n",
    "    # 3.4 Off-policy prediction: e.g. set all discounts to 30% or 10%\n",
    "    for disc in [0.1, 0.3, 0.5]:\n",
    "        y_pred_off = forecaster.predict(test_df, discount_scenario=disc)\n",
    "        mae_off = mean_absolute_error(y_true, y_pred_off)\n",
    "        mse_off = mean_squared_error(y_true, y_pred_off)\n",
    "        print(f\"\\nOff-policy performance (counterfactual discount = {disc:.1f}):\")\n",
    "        print(f\"  MAE vs actual realized demand = {mae_off:.3f}\")\n",
    "        print(f\"  MSE vs actual realized demand = {mse_off:.3f}\")\n",
    "\n",
    "    # You can also inspect how average predicted demand changes with discount:\n",
    "    for disc in [0.0, 0.2, 0.4, 0.6]:\n",
    "        y_pred = forecaster.predict(test_df, discount_scenario=disc)\n",
    "        print(\n",
    "            f\"Average predicted demand at discount {disc:.1f}: \"\n",
    "            f\"{y_pred.mean():.2f}\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databricks-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
